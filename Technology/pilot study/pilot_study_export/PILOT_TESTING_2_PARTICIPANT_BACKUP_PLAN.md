# Backup Plan: 2-Participant Pilot Testing
## Human-AI Collaborative Poetry Writing Research Platform

**Study Date:** November 2025  
**Number of Participants:** 2 (Backup/Minimum Viable)  
**Duration:** ~35 minutes per participant (split into 2 sessions)  
**Study Type:** Exploratory usability pilot with parameter comparison

---

## ğŸ¯ Purpose of This Backup Plan

This document provides a **contingency protocol** if you can only recruit **2 participants** instead of the planned 4.

**When to use this plan:**
- âœ… Recruitment challenges limit participant availability
- âœ… Timeline constraints require faster pilot completion
- âœ… Budget limitations reduce feasible sample size
- âœ… Want to test core system functionality before expanding

**Key Difference from Original:**
- **Focus**: Technical validation & usability testing (not full factorial research design)
- **Goal**: Verify platform works and gather qualitative insights
- **Outcome**: System improvements + readiness assessment for full study

---

## 1. Research Design (2-Participant Version)

### 1.1 Simplified Research Questions

**Primary Questions (Technical):**
1. Does the platform function reliably for poetry writing sessions?
2. Can participants successfully interact with the AI to create poems?
3. What usability issues need addressing before full deployment?

**Secondary Questions (Exploratory):**
1. How do structured vs. exploratory AI parameters affect user experience?
2. What interaction patterns emerge in human-AI poetry collaboration?
3. Which poetry forms do participants engage with?

### 1.2 Study Conditions (2 Rooms)

**OPTION 1: Parameter Comparison (Recommended)**

| Participant | Room ID | AI Style | Temperature | Top-P | Awareness | Focus |
|-------------|---------|----------|-------------|-------|-----------|-------|
| **P1** | `room_1_structured_unaware` | Structured | 0.25 | 0.35 | Unaware | Consistent, rule-based guidance |
| **P2** | `room_2_exploratory_unaware` | Exploratory | 0.80 | 0.90 | Unaware | Creative, varied suggestions |

**Rationale:**
- Tests **core hypothesis**: Do parameter settings affect collaboration quality?
- Isolates parameter variable (awareness held constant)
- Provides maximum contrast between conditions
- Most valuable comparison for full study design

---

### Alternative Options (If Different Focus Needed)

**OPTION 2: Awareness Comparison**

| Participant | Room ID | AI Style | Temperature | Top-P | Awareness |
|-------------|---------|----------|-------------|-------|-----------|
| P1 | `room_3_exploratory_aware` | Exploratory | 0.80 | 0.90 | **Aware** |
| P2 | `room_4_exploratory_unaware` | Exploratory | 0.80 | 0.90 | **Unaware** |

**Use if:** UI transparency is your primary concern

**OPTION 3: Maximum Contrast**

| Participant | Room ID | AI Style | Temperature | Top-P | Awareness |
|-------------|---------|----------|-------------|-------|-----------|
| P1 | `room_1_structured_aware` | Structured | 0.25 | 0.35 | Aware |
| P2 | `room_4_exploratory_unaware` | Exploratory | 0.80 | 0.90 | Unaware |

**Use if:** Want to test extreme differences to identify major issues

---

## 2. Adjusted Testing Schedule

### Week 1: Preparation (2 days instead of 4)

**Day 1:**
- Morning: Recruit 2 participants, explain study
- Afternoon: Confirm schedules, send calendar invites
- Evening: Final system testing

**Day 2:**
- Prepare materials (consent forms, questionnaires Ã— 2)
- Set up testing space
- Test both room configurations

### Week 2: Data Collection

**Day 1 (Session 1 - Both Participants):**
- 9:00-9:10 AM: **P1 Session 1** (Login & Setup)
- 9:30-9:40 AM: **P2 Session 1** (Login & Setup)
- Afternoon: Review any technical issues, fix if needed

**Day 2-3 (Session 2 - Poetry Writing):**
- **Day 2, 10:00-10:25 AM**: **P1 Session 2** (Structured Room)
- **Day 3, 10:00-10:25 AM**: **P2 Session 2** (Exploratory Room)

*Note: 1-day gap between participants allows immediate fixes if P1 encounters issues*

### Week 3: Analysis & Reporting
- Day 1-2: Data extraction and qualitative analysis
- Day 3: Write pilot report with system improvement recommendations
- Day 4: Decision point - recruit 2 more participants or proceed to full study?

**Total Duration: 2-3 weeks** (vs. 4 weeks for 4 participants)

---

## 3. Participant Recruitment (2-Person Strategy)

### 3.1 Modified Recruitment Approach

**Target Profile:**
- 1 participant with **some** poetry experience (for structured room)
- 1 participant with **minimal** poetry experience (for exploratory room)
- Both: English L2 learners, B1+ proficiency, comfortable with technology

**Recruitment Channels (Prioritized for Speed):**
1. **Direct personal contacts** - fastest method
2. **Course announcement** in English classes
3. **Social media** in student groups
4. **Email to departmental mailing list**

### 3.2 Simplified Recruitment Script

> "I'm testing a new AI poetry writing tool and need 2 volunteers for a quick pilot study. You'll do two short sessions (10 min + 25 min) and receive **$25**. No poetry experience needed! Want to help test it out?"

**Follow-up:**
- "Session 1 is just 10 minutes to set up login"
- "Session 2 is 25 minutes creating poetry with AI assistance"
- "Can schedule both this week - super flexible!"

### 3.3 Backup Recruitment Options

**If struggling to find 2 participants:**
1. Increase incentive to $30-40 per participant
2. Offer course extra credit (if in academic setting)
3. Include lunch/coffee during session
4. Reduce to 1 participant (see Section 12 for 1-person protocol)

---

## 4. Two-Session Structure (Same as Original)

### SESSION 1: Login & Setup (10 minutes)

| Time | Activity | Duration |
|------|----------|----------|
| 0:00-0:03 | Welcome & Consent | 3 min |
| 0:03-0:06 | Pre-session Questionnaire | 3 min |
| 0:06-0:09 | Login Testing | 3 min |
| 0:09-0:10 | Schedule Session 2 | 1 min |

**Can run both P1 and P2 on same day** (stagger by 30 minutes)

---

### SESSION 2: Poetry Writing (25 minutes)

| Time | Activity | Duration |
|------|----------|----------|
| 0:00-0:02 | Welcome Back & Login | 2 min |
| 0:02-0:04 | Platform Tutorial | 2 min |
| 0:04-0:19 | Poetry Writing Session | 15 min |
| 0:19-0:24 | Post-session Interview | 5 min |
| 0:24-0:25 | Debrief & Thank You | 1 min |

**Run P1 and P2 on different days** (allows fixes between sessions)

---

## 5. Enhanced Data Collection (Qualitative Focus)

### 5.1 Automatic System Logging (Same)
- âœ… All conversation messages
- âœ… Poetry form selections
- âœ… Session timestamps
- âœ… Interaction type classifications

### 5.2 Pre-Session Questionnaire (Same)
Use standard questionnaire from original plan

### 5.3 **EXTENDED** Post-Session Interview (10 minutes instead of 5)

**With only 2 participants, spend more time on qualitative depth!**

#### Core Questions (5 min):
1. "Walk me through your experience creating poetry with the AI. What was it like?"
2. "What worked well? What was frustrating or confusing?"
3. "Did you feel the AI helped or hindered your creativity? How?"
4. "Would you use a tool like this again for creative writing? Why or why not?"

#### Deep-Dive Questions (5 min):
5. "Describe a specific moment when the AI surprised you - good or bad."
6. "How would you compare this to writing poetry on your own?"
7. "If you could change one thing about the interface or AI, what would it be?"
8. "Did you feel like you maintained your own creative voice, or did the AI dominate?"
9. "What kind of writer do you think would benefit most from this tool?"
10. "Any technical issues or confusion points I should know about?"

### 5.4 **NEW: Think-Aloud Protocol**

During the 15-minute writing session, occasionally prompt:
- "Can you tell me what you're thinking right now?"
- "What are you deciding between?"
- "How do you feel about that AI response?"

**Document these spontaneous insights!**

### 5.5 **NEW: Post-Session Written Reflection**

Email participants 2-3 days after Session 2:

> "Thank you again for participating! I'd love one final reflection: After a few days, what's your biggest takeaway from the AI poetry writing experience? Any new thoughts? (2-3 sentences fine)"

### 5.6 Detailed Observational Notes

**Facilitator should note:**
- **Engagement markers**: leaning in, smiling, frowning, sighing
- **Confusion points**: hesitation, re-reading, asking for help
- **Spontaneous comments**: "Oh interesting!", "That doesn't make sense", etc.
- **Pacing**: fast typing vs. long pauses
- **Form selection**: How long to choose? Change forms?
- **Technical issues**: Any glitches, slowness, errors

---

## 6. Data Analysis Plan (2-Participant Focus)

### 6.1 Quantitative Metrics (Descriptive Only)

**Do NOT attempt statistical significance testing with N=2**

**Instead, create comparison tables:**

| Metric | P1 (Structured) | P2 (Exploratory) | Observation |
|--------|----------------|-----------------|-------------|
| Session duration | [X] min | [Y] min | Longer/shorter? |
| Messages exchanged | [X] | [Y] | More/fewer interactions? |
| User vs AI ratio | [X]% | [Y]% | Who talks more? |
| Poems created | [X] | [Y] | Completion rate |
| Forms selected | [List] | [List] | Preferences differ? |
| Type A interactions | [X] | [Y] | More guided? |
| Type B interactions | [X] | [Y] | More collaborative? |
| Type C interactions | [X] | [Y] | More exploratory? |

**Report as**: "P1 sent 23 messages vs P2's 31 messages, suggesting [observation]"

---

### 6.2 Qualitative Analysis (Primary Focus)

#### Thematic Coding of Interviews

**Create codes for:**
1. **User Experience**
   - Positive sentiments (enjoyment, surprise, satisfaction)
   - Negative sentiments (frustration, confusion, disappointment)
   - Neutral observations

2. **AI Partnership Perception**
   - AI as assistant vs. collaborator vs. dominator
   - Trust indicators
   - Creative control feelings

3. **Usability Issues**
   - Interface confusion points
   - Technical problems
   - Desired features
   - Workflow disruptions

4. **Creative Process Impact**
   - Inspiration sources
   - Decision-making patterns
   - Revision behaviors
   - Creative voice preservation

#### Compare Between Participants

**Create comparison table:**

| Theme | P1 (Structured) | P2 (Exploratory) |
|-------|----------------|-----------------|
| Overall sentiment | [Quote] | [Quote] |
| AI role perception | [Description] | [Description] |
| Main frustration | [Quote] | [Quote] |
| Main delight | [Quote] | [Quote] |
| Usability issue | [Description] | [Description] |

---

### 6.3 Conversation Content Analysis

**Deeply analyze the chat transcripts:**

1. **Turn-taking patterns**
   - Who initiates topics?
   - How many exchanges per poem?
   - User leading vs AI leading?

2. **Language analysis**
   - Vocabulary diversity (use word clouds)
   - Poetic devices used
   - Emotional tone

3. **Interaction sequences**
   - Most common patterns (e.g., requestâ†’suggestionâ†’revision)
   - Successful vs unsuccessful exchanges
   - How participants recover from confusion

4. **Parameter effects** (P1 vs P2)
   - Does structured AI produce more consistent responses?
   - Does exploratory AI introduce more variety?
   - Which generates better user engagement?

---

### 6.4 Poetry Output Analysis

**Compare the poems created:**

| Aspect | P1 Poems | P2 Poems |
|--------|----------|----------|
| Number completed | [X] | [Y] |
| Forms chosen | [List] | [List] |
| Word count avg | [X] | [Y] |
| Imagery/metaphors | [Count/describe] | [Count/describe] |
| Emotional tone | [Describe] | [Describe] |
| Quality assessment | [1-5 rating + notes] | [1-5 rating + notes] |

**Get external blind rating**: Ask 2 poetry teachers to rate poems (1-5) without knowing condition.

---

## 7. Modified Success Criteria

### 7.1 Technical Success (Primary Goal)
- âœ… Both participants complete sessions without major crashes
- âœ… Authentication system works reliably
- âœ… Data successfully logged to database
- âœ… Export functions produce usable files
- âœ… Both room configurations function as designed

### 7.2 Usability Success
- âœ… Identify at least 3 concrete UI/UX improvements
- âœ… No critical confusion that blocks task completion
- âœ… Participants can independently navigate interface after 2-min tutorial
- âœ… Average engagement rating > 3/5

### 7.3 Research Readiness Success
- âœ… Confirm that parameter settings produce noticeably different AI behavior
- âœ… Poetry writing task is feasible in 15 minutes
- âœ… Data collection methods capture necessary information
- âœ… Interview questions elicit useful insights

### 7.4 Participant Experience Success
- âœ… Both participants complete all tasks
- âœ… At least 1 participant reports positive experience
- âœ… Each participant creates at least 1 poem
- âœ… No ethical concerns or distress

---

## 8. Session Protocols (Adapted for 2 Participants)

### 8.1 SESSION 1: Login & Setup

**Same as original 4-participant plan**, but:

#### Scheduling Tip for 2 Participants:
- Can schedule **both P1 and P2 on same morning**:
  - 9:00-9:10 AM: P1 Session 1
  - 9:30-9:40 AM: P2 Session 1
- This allows you to fix any issues before Session 2 (days later)

#### Modified Script Addition:

**After consent:**
> "Quick note: You're one of just two pilot participants helping us test this system before a larger study. Your feedback will be especially valuable! Please feel free to share any and all observations."

---

### 8.2 SESSION 2: Poetry Writing

**Enhanced observation protocol for 2 participants:**

#### During 15-Minute Writing Session:

**Every 3-5 minutes, silently note:**
- Current activity (reading AI response, typing, thinking)
- Facial expression (engaged, confused, frustrated, delighted)
- Interaction pace (flowing, hesitant, stuck)

**At 8-minute mark:** Gentle check-in
> [If participant seems stuck] "How's it going? Feel free to try a different approach or form if you want."

**At 12-minute mark:** Time warning
> "About 3 minutes left - feel free to wrap up your current poem or start wrapping up."

#### Enhanced Interview Protocol (10 minutes)

**Opening:**
> "Thank you for that session! I'd love to hear your thoughts. Since you're one of just two pilot testers, your detailed feedback is crucial."

**Core Questions (5 min):**
1. Experience overview
2. What worked / what didn't
3. AI help vs hindrance
4. Future use likelihood

**Deep Dive (5 min):**
5. "Tell me about one specific moment - maybe when the AI said something that really worked or really didn't work."
   - [Follow up]: "Why did that stand out?"

6. "How did working with AI compare to writing poetry alone or with a human partner?"

7. "If I could wave a magic wand and change one thing about this tool, what should it be?"

8. "Did you feel like the AI 'got' what you were trying to create?"
   - [Follow up]: "Can you give an example?"

9. "What type of student or writer would benefit most from this? Who might struggle?"

10. "Any technical glitches, confusing instructions, or moments you felt lost?"

**Record detailed responses** - these 2 perspectives are your entire dataset!

---

## 9. Adjusted Budget

### 9.1 Cost Breakdown

| Item | Original (4p) | 2-Participant | Savings |
|------|--------------|--------------|---------|
| Participant incentives | $100 (4Ã—$25) | **$50** (2Ã—$25) | $50 |
| OpenRouter API usage | $10 | **$5** | $5 |
| Materials (printing) | $20 | **$15** | $5 |
| **Total** | **$130** | **$70** | **$60** |

### 9.2 Optional Enhancement (Use Savings)

**Consider upgrading incentive:**
- **$30-35 per participant** (instead of $25)
- Shows appreciation for being pilot testers
- Increases recruitment likelihood
- Still under original budget ($60-70 vs $100)

---

## 10. Risk Mitigation for Small Sample

### 10.1 Risks of N=2

**Statistical Limitations:**
- âŒ Cannot test significance
- âŒ Cannot generalize findings
- âŒ High susceptibility to individual variation
- âŒ One dropout = 50% data loss

**Mitigation Strategies:**

1. **Over-recruit if possible**
   - Recruit 3 participants, use 2 (1 backup)
   - Or recruit 2 with 1 backup on standby

2. **Frame as "Usability Pilot" not "Research Pilot"**
   - Explicitly state: "This is technical validation"
   - Position findings as exploratory, not confirmatory

3. **Maximize data depth**
   - Longer interviews (10 min vs 5 min)
   - Think-aloud protocol during session
   - Follow-up reflection emails
   - External poem quality ratings

4. **Supplement with facilitator expertise**
   - Your observations as experienced researcher matter
   - Document your impressions systematically
   - Compare to literature expectations

5. **Plan for second pilot round**
   - Use insights to improve system
   - Recruit 2 more participants (total N=4)
   - Now you have the full 2Ã—2 design!

---

### 10.2 What if One Participant Drops Out?

**If dropout happens BEFORE Session 1:**
- Recruit replacement immediately
- Delay pilot by a few days if needed

**If dropout happens AFTER Session 1 but BEFORE Session 2:**
- You have their questionnaire data
- Proceed with 1 participant (see Section 12)
- Or recruit 1 replacement for that room condition

**If dropout happens AFTER Session 2:**
- You have complete data for 1 participant
- Conduct **case study analysis** (see Section 12)

---

## 11. Reporting for 2-Participant Pilot

### 11.1 Report Structure

#### Title:
**"Technical Pilot Study Report: AI-Assisted Poetry Writing Platform (N=2)"**

#### Abstract:
> "This exploratory pilot tested the technical functionality and usability of an AI poetry writing platform with 2 participants (structured vs exploratory AI parameters). Primary goals were system validation and identification of improvement areas before full-scale deployment. Findings indicate [brief summary]."

#### Method:
- Clearly state: **N=2, non-generalizable, exploratory**
- Describe participants (P1, P2)
- Explain room conditions tested
- Detail data collection methods

#### Results:
**Present as comparative case studies:**

##### Participant 1 (Structured AI):
- Session metrics: [data]
- Experience description: [qualitative]
- Key quotes: [3-4 illustrative quotes]
- Poems created: [brief description]

##### Participant 2 (Exploratory AI):
- Session metrics: [data]
- Experience description: [qualitative]
- Key quotes: [3-4 illustrative quotes]
- Poems created: [brief description]

##### Comparison:
- Use comparison tables (see Section 6)
- Highlight differences and similarities
- Report as **observations**, not statistical findings

#### Technical Findings:
- System bugs encountered: [list]
- Usability issues: [list with severity ratings]
- Successful features: [list]

#### Discussion:
- What worked well
- What needs improvement
- Implications for full study design
- Limitations of N=2

#### Recommendations:
1. **Immediate fixes needed** (before full study)
2. **Consideration points** (monitor in full study)
3. **Research design adjustments** (sample size, measures, conditions)

#### Conclusion:
- Platform readiness assessment
- Go/no-go decision for full study
- Next steps

---

### 11.2 Sample Findings Statement

**Example of appropriate N=2 reporting:**

> "Participant 1, using the structured AI (temp=0.25), completed 2 haiku poems and reported feeling 'guided' by the AI's consistent suggestions. In contrast, Participant 2, with exploratory AI (temp=0.80), created 1 longer free-verse poem and described the experience as 'surprising' and 'sometimes confusing but exciting.' While these contrasts suggest potential parameter effects, **the small sample size (N=2) prevents any generalization**. These observations will inform hypothesis development for the full study."

**Always include disclaimers like:**
- "Given the small sample..."
- "These exploratory findings suggest..."
- "While not generalizable..."
- "As a technical pilot..."

---

## 12. Contingency: If Only 1 Participant Available

### 12.1 Single-Participant Protocol

**If you truly can only recruit 1 person:**

#### Option A: Serial Testing (Recommended)
Have the 1 participant test **both** conditions:

**Week 1:**
- Session 1: Login & setup
- Session 2: Room 1 (Structured AI) - Create 1-2 poems

**Week 2 (7 days later):**
- Session 3: Room 2 (Exploratory AI) - Create 1-2 poems
- Extended interview comparing both experiences

**Incentive:** $40-50 (higher because more time commitment)

**Advantages:**
- Within-subject comparison
- Same person = controls for individual differences
- Still tests both parameter conditions

**Disadvantages:**
- Order effects (always structured first)
- Familiarity effects (second session easier)
- Longer timeline

---

#### Option B: Deep Dive Case Study
Use 1 participant for intensive study:

**Structure:**
- Session 1: Login & questionnaire (10 min)
- Session 2: Poetry writing (25 min) in ONE condition
- Session 3 (same day, 2 hours later): Extended interview (30 min)
- Follow-up: Written reflection + email interview

**Focus:**
- Extremely detailed qualitative analysis
- Think-aloud protocol throughout
- Screen recording + audio recording
- Member-checking (show them analysis, get feedback)

**Report as:**
- "Instrumental case study" (Stake, 1995)
- "Single-subject usability evaluation"
- "Deep exploratory pilot"

---

### 12.2 Reframe Your Pilot Goals

**If N=1, adjust expectations entirely:**

**Primary Goal:** Technical validation only
- Does the system work end-to-end?
- What bugs exist?
- Is the task feasible?

**Secondary Goal:** Generate hypotheses
- What patterns emerge?
- What questions should full study ask?
- What measures are most informative?

**NOT Goals:**
- âŒ Test research hypotheses
- âŒ Compare conditions
- âŒ Validate design choices

---

## 13. Decision Tree: How Many Participants Can You Get?

```
Can you recruit participants?
â”‚
â”œâ”€ 4+ participants â†’ Use ORIGINAL 4-participant plan
â”‚                     (Full 2Ã—2 factorial design)
â”‚
â”œâ”€ 2-3 participants â†’ Use THIS BACKUP PLAN (Option 1)
â”‚                      (Parameter comparison focus)
â”‚
â”œâ”€ 1 participant â†’ Use Section 12 protocols
â”‚                   (Serial testing or case study)
â”‚
â””â”€ 0 participants â†’ Self-testing by research team
                    (Technical validation only)
                    Then recruit for next round
```

---

## 14. Moving Forward After 2-Participant Pilot

### 14.1 Immediate Post-Pilot Actions

**Within 24 hours:**
- [ ] Export all data from admin dashboard
- [ ] Back up raw data files (local + cloud)
- [ ] Transcribe interview audio (if recorded)
- [ ] Write up immediate observations while fresh

**Within 1 week:**
- [ ] Complete data analysis (Section 6)
- [ ] Identify system bugs and fixes needed
- [ ] Draft pilot report (Section 11)
- [ ] List UI/UX improvements

**Within 2 weeks:**
- [ ] Implement critical bug fixes
- [ ] Make priority UI improvements
- [ ] Revise session protocols if needed
- [ ] **Decision point**: Ready for full study?

---

### 14.2 Decision Point: Next Steps

#### OPTION A: Proceed to Full Study
**Choose if:**
- âœ… No critical technical issues found
- âœ… Participants had positive experiences
- âœ… Data collection methods worked well
- âœ… Clear differences observed between conditions
- âœ… You have budget/timeline for larger study

**Next:** Recruit 16-20 participants for full study

---

#### OPTION B: Run Second 2-Participant Pilot
**Choose if:**
- âš ï¸ Major system improvements made after first pilot
- âš ï¸ Need to test revised protocols
- âš ï¸ Want full 2Ã—2 design before committing to large study
- âš ï¸ Budget allows ($70 more)

**Next:** Recruit 2 more participants for other 2 conditions
- P3: Exploratory-Aware
- P4: Structured-Aware

**Now you have N=4** (all conditions tested)

---

#### OPTION C: Pivot to Alternative Design
**Choose if:**
- âŒ Fundamental issues with parameter approach
- âŒ Poetry writing task not feasible as designed
- âŒ Need to reconsider research questions

**Next:** Redesign study based on pilot insights

---

### 14.3 Full Study Planning

**If proceeding to full study, use pilot findings to:**

1. **Finalize sample size:**
   - Target: 16-24 participants (4-6 per condition)
   - Power analysis if possible
   - Account for ~20% attrition

2. **Refine research questions:**
   - What did pilot suggest is most interesting?
   - What differences emerged?
   - What new questions arose?

3. **Optimize data collection:**
   - Drop measures that didn't work
   - Add measures for unexpected patterns
   - Adjust interview questions

4. **Improve system:**
   - Fix all identified bugs
   - Implement high-priority UI changes
   - Optimize performance

5. **Update protocols:**
   - Clarify confusing instructions
   - Adjust timing if needed
   - Add facilitator training notes

---

## 15. Quick Reference Checklist

### Pre-Pilot (2 Participants)
- [ ] Recruit 2 participants (or 3 with 1 backup)
- [ ] Schedule Session 1 for both (same day OK)
- [ ] Schedule Session 2 (different days, 1-3 days after Session 1)
- [ ] Test Room 1 (Structured) configuration
- [ ] Test Room 2 (Exploratory) configuration
- [ ] Prepare materials (Ã—2): consent forms, questionnaires, interview guides
- [ ] Set up testing environment
- [ ] Charge recording devices
- [ ] Verify API credits sufficient
- [ ] Prepare $25 incentives (Ã—2)

### Session 1 (Each Participant - 10 min)
- [ ] Welcome & explain two-session structure
- [ ] Sign consent form
- [ ] Complete pre-session questionnaire
- [ ] Test login (phone authentication)
- [ ] Schedule Session 2
- [ ] Fix any issues before P2's Session 1

### Session 2 (Each Participant - 25 min)
- [ ] Welcome back & independent login
- [ ] 2-minute platform tutorial
- [ ] **15 minutes: Poetry writing** (observe & take notes!)
- [ ] 10-minute extended interview (record!)
- [ ] Brief debrief & provide $25
- [ ] Send follow-up reflection email (2-3 days later)

### Post-Pilot Analysis
- [ ] Export all data immediately
- [ ] Backup files (local + cloud)
- [ ] Transcribe interviews
- [ ] Create comparison tables (Section 6.1)
- [ ] Code qualitative themes (Section 6.2)
- [ ] Analyze conversation transcripts (Section 6.3)
- [ ] Evaluate poetry outputs (Section 6.4)
- [ ] List technical bugs found
- [ ] List usability issues identified
- [ ] Write pilot report (Section 11)
- [ ] **Decision: Next steps?** (Section 14.2)

---

## 16. Key Differences from 4-Participant Plan

### What's the Same:
- âœ… Two-session structure (10 min + 25 min)
- âœ… Session protocols and scripts
- âœ… Room configurations (just using 2 of 4)
- âœ… Data collection methods
- âœ… Poetry writing task and duration

### What's Different:
- ğŸ”„ **Sample size**: 2 instead of 4 participants
- ğŸ”„ **Focus**: Technical validation > hypothesis testing
- ğŸ”„ **Conditions tested**: Only 2 rooms (structured vs exploratory)
- ğŸ”„ **Analysis approach**: Qualitative comparison, no statistics
- ğŸ”„ **Interview length**: 10 minutes (deeper dive)
- ğŸ”„ **Reporting**: Case study format, not experimental
- ğŸ”„ **Success criteria**: System functionality > research findings
- ğŸ”„ **Budget**: $70 instead of $130
- ğŸ”„ **Timeline**: 2-3 weeks instead of 4 weeks

### What's Enhanced:
- âœ¨ Extended interview (10 min vs 5 min)
- âœ¨ Think-aloud protocol during session
- âœ¨ Post-session written reflection
- âœ¨ Deeper qualitative analysis
- âœ¨ More detailed observational notes
- âœ¨ Option for second pilot round

---

## 17. Final Tips for Success

### 17.1 Make the Most of 2 Participants

1. **Treat them as co-researchers**
   - "You're helping us design this system"
   - "Your feedback shapes the next version"
   - Ask: "What would make this better?"

2. **Go deep, not broad**
   - Prioritize rich qualitative data
   - Follow up on interesting comments
   - Don't rush through interviews

3. **Document everything**
   - Video record if possible (with consent)
   - Take detailed field notes
   - Save all artifacts

4. **Be transparent about limitations**
   - In reports, acknowledge N=2
   - Frame as exploratory/technical pilot
   - Don't overstate findings

5. **Use pilot to improve, not prove**
   - Goal = better system and design
   - Not: confirm hypotheses
   - Focus on learning

### 17.2 Maximizing Value

**Compensate for small N by:**
- âœ… Longer interviews (10 min vs 5 min)
- âœ… Multiple data sources (chat logs + interviews + observations + poems + reflections)
- âœ… Member checking (show findings to participants)
- âœ… External review (show poems to poetry instructors)
- âœ… Iterative testing (2 participants â†’ fixes â†’ 2 more â†’ full study)

### 17.3 When to Pivot

**Stop and reconsider if:**
- âŒ Both participants have strongly negative experiences
- âŒ Technical failures prevent task completion
- âŒ Participants can't understand instructions despite help
- âŒ No discernible difference between conditions
- âŒ Fundamental design flaws identified

**These are valuable findings!** Better to discover now than with 20 participants.

---

## 18. Contact & Support

**Questions about this backup plan?**
- Review Section 14.2 for decision tree
- Compare to original 4-participant plan
- Consider hybrid approach (2 now, 2 later)

**Stuck during pilot?**
- Refer to troubleshooting (original plan Section 7)
- Document issues for post-pilot improvements
- Don't abandon session unless safety concern

**After pilot completion?**
- Follow Section 14 decision tree
- Write honest assessment of readiness
- Celebrate that you tested before scaling!

---

## Conclusion

**This 2-participant backup plan allows you to:**
- âœ… Validate technical functionality
- âœ… Test core parameter comparison
- âœ… Gather usability feedback
- âœ… Identify system improvements
- âœ… Assess readiness for full study
- âœ… Stay within budget ($70)
- âœ… Complete in 2-3 weeks

**Remember:**
- N=2 is **sufficient for usability piloting**
- N=2 is **insufficient for research claims**
- Frame appropriately and extract maximum value
- Use findings to improve before scaling up

**Good luck with your pilot! ğŸš€**

---

**Document Version:** 1.0  
**Created:** November 10, 2025  
**Purpose:** Backup contingency plan for 2-participant pilot testing  
**Use in conjunction with:** Original 4-participant pilot testing scheme

